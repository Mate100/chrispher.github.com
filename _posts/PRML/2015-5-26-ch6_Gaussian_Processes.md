---
layout: post
title: ch6-高斯过程
category: 机器学习
tags: [机器学习, PRML]
path: /notes/PRML/2015-5-26-ch6_Gaussian_Processes.md
description: 介绍了高斯过程
---

本章主要是介绍了核方法和高斯过程的一些知识。笔记分两部分，这是第一部分：高斯过程。

<!-- more -->

注：本文仅属于个人学习记录而已！参考Chris Bishop所著[Pattern Recognition and Machine Learning(PRML)](http://research.microsoft.com/en-us/um/people/cmbishop/PRML/)以及由Elise Arnaud, Radu Horaud, Herve Jegou, Jakob Verbeek等人所组织的Reading Group。

###目录
{:.no_toc}

* 目录
{:toc}

###1、前言
现在我们扩展一下核方法，把它应用在概率判别模型中，可以得到我们这部分的主题——高斯过程(Gaussian Processes).在第三章的线性回归模型中，$$y(x,w) = w^T \Phi(x) $$，这里w是参数向量，$$\Phi(x)$$是基于输入向量x的非线性变换得到的向量。我们引入的w先验分布，在根据训练集，估计得到参数w的后验分布，之后可以得到回归函数的后验分布。那么可以得到预测分布$$p(t \mid x)$$。

从高斯过程的观点看，我们不使用参数模型（不是引入参数的先验分布），而是直接定义回归函数的一个先验分布。即认为函数在函数空间里存在一个先验分布。初步看，回归函数是不可胜数的，那么先验分布也就很难得到。但是，后面会具体提到。此外，很多等价于高斯过程的模型都有很广泛的应用。比如ARMA(autoregressive moving average)等等。

###2、高斯过程引入
我们再看一下线性回归，重新推导一下预测分布，将会看到这是高斯过程的一个特例。考虑线性回归模型由M个基函数构成，即$$y(x) = w^T \Phi(x)$$，引入w的先验分布$$p(w) = N(w \mid 0, \alpha^ {-1} I)$$。根据训练数据$$x_1, ..., x_N$$来估计y(x)的分布，联合分布是$$y=\Phi w$$，其中$$y_n = y(x_n) = w^T \Phi(x_n)$$，也是高斯分布，那么我们只需要知道分布的均值和方差即可。如下：

$$E[y] = \Phi E[w] = 0$$
$$cov[y] = E[y y^T] = \Phi E[w w^T] \Phi = \frac{1}{\alpha} = K$$

这里的K是高斯矩阵，元素是$$K_ {nm} = k(x_n, x_m) = \frac{1}{\alpha} \Phi(x_n) ^T \Phi(x_m)$$,这里的$$k(x, x')$$是核函数。截至目前，我们只是了先验知识和训练数据，没有使用目标值。

这个模型是高斯分布的一个特例。通常来说，高斯分布是指在函数y(x)上的先验分布，而y(x)值，由任意集合点$$x_1, ..., x_N$$的联合分布得到，且y(x)的值服从高斯分布。高斯随机过程的核心是，N个变量$$y_1, ..., y_N$$的联合分布完全由两次统计得到，即均值和方差。在多数应用中，我们不需要有任何关于y(x)均值的先验知识，可以简单的认为是0，这等价于在基函数观点下回归参数的先验分布的均值为0。高斯过程则完全由y(x)的方差决定，使用核方法 $$E(y(x_n) y(x_m)) = k(x_n, x_m)$$。我们也可以直接定义和核函数。

###3、高斯过程回归
为了在回归过程中应用高斯过程，我们考虑观测目标值的噪声的影响，即$$t_n = y_n + \epsilon$$，这里的$$\epsilon$$是指噪声，不同的观测下是独立的。假设噪声服从高斯分布，那么$$p(t_n \mid y_n) = N(t_n \mid y_n, \beta^ {-1} )$$,这里的$$\beta$$是超参数，控制着噪声的精度。考虑所有的观测值，有$$p(t \mid y) = N(t \mid y, \beta ^{-1} T_N)$$。根据高斯过程的定义，边际分布p(y)是高斯分布，均值为0，方差由Gram矩阵K决定，即$$p(y) = N(y \mid 0, K)$$。而核函数决定着K，用来表达点$$x_n$$和$$x_m$$的相似度。接下来，可以得到t的边际分布如下：

$$p(t) = \int p(t \mid y) p(y) dy = N(t \mid 0, C)$$

这里的方差矩阵C，元素值为$$C(x_n, x_m) = k(x_n, x_m) + \beta ^ {-1} \epsilon_ {nm}$$。这表明方差主要来自y(x)和噪声，由于他们独立可以直接相加。

高斯过程回归中一个广泛使用的核函数是$$k(x_n, x_m) = \theta_0 exp(-\frac{\theta_1}{2} \mid \mid x_n - x_m \mid \mid ^2) + \theta_2 + \theta_3 x_n^T x_m$$

目前为止，我们使用高斯过程都在建立基于训练数据联合分布的模型。而我们的目标是给定一个新的输入$$x_ {N+1}$$得到一个预测值，这要求我们估计预测分布$$p(t_ {N+1 \mid t_N})$$，需要注意该分布仍然是在变量$$x_1, ..., x_N$$的条件下的，我们这里只是为了简化表达，而$$t_N = (t_1, ..., t_N)^T, t_ {N+1} = (t_1, ..., t_ {N+1})^T$$。

根据上面p(t)服从高斯分布，那么$$p(t_{N+1} = N( t_ {N+1} \mid 0, C_ {N+1}))$$，这里的$$C_ {N+1}$$ 是一个N+1维的方差矩阵。因为联合分布是高斯分布，根据之前的结论，我们可以的得到:

$$C_ {N+1} = \begin{bmatrix} C_N & k \\\\ k^T & c \end{bmatrix}$$

最后我们可以得到:

$$m(x_ {N+1}) = k^T C^ {-1} _N t$$

$$\sigma^2 (x_ {N+1}) = c - k^TC^ {-1} _N k$$

这里我们的向量k是新输入$$x_ {N+1}$$的函数，所以预测分布的均值和方差均依赖于$$x_ {N+1}$$。这里唯一的要求是核函数是方差矩阵，必须是正定的。注意这里的均值也可以写为$$x_ {N+1}$$的函数，即$$m(x_ {N+1}) = \sum^N_ {n=1} a_n k(x_n, x_ {N+1})$$，这里的$$a_n$$是$$C_N^ {-1} t$$的第n个元素。再如果我们的核函数只依赖于距离$$\mid \mid x_n - x_m \mid \mid$$，那么可以得到radial基函数的扩展。

在上面的几个例子中，核函数$$k(x_n, x_m)$$都是任意定义的。在实际应用中，一般都是有限个基函数组成。对于这些模型，我们可以用不同的方式来看待预测分布，既可以从参数空间(parameter space)，使用线性回归的观点，也可以从基于高斯过程的函数空间(function space)的观点。最后，需要注意下效率的问题。使用高斯过程中，需要计算维度是N的矩阵的逆矩阵，计算复杂度是$$O(N^3)$$，而使用基函数模型，我们需要计算维度是M的$$S_N$$的逆矩阵，计算复杂度是$$O(M^3)$$。对于一个新的预测值，高斯过程需要$$O(N^2)$$, 而线性基函数模需要$$O(M^2)$$。一般而言，基函数个数M小于样本数N，因此高斯过程是比较费时。然而高斯过程能够用有限的基函数来表达协方差函数。

对于训练数据集很大的问题，直接应用高斯过程方法是不实际的。因此，有很多近似的方法，用于优化训练数据个数。实际问题的讨论，可以参考Bishop的论文。高斯过程还可以扩展成多个目标值的预测，称之为co-kriging, 也要一些扩展使其应用于非监督学习等等。

在高斯过程预测中，很大都依赖于协方差函数的选择。实际中，我们会选择带有参数的函数族，而非一个协方差函数，之后只要从数据中学习这些参数即可。这些参数决定着协方差的长度以及噪声的精度，在一个参数模型中相当于超参数。我们可以使用最大似然法来学习这些参数，似然函数$$p(t \mid \theta)$$，其中$$\theta$$是高斯过程的超参数。因为$$\theta$$是超参数，因此可以看成是2个最大似然的过程，使用基于梯度的一些优化方法即可。书中也提及了一个问题，就是噪声的精度是依赖于训练数据的情况，需要引入第二个高斯过程来建模$$\ln \beta$$。

上面提到的用最大似然法来决定协方差参数的方法，可以扩展用于输入变量（对应不同的参数）。通过最大似然来优化这些参数，可以得到这些输入的相对重要性，这种方法称之为自动相关判定(ARD, automatic relevance determination)。具体的会在后面提到。

对于二维输入空间里$$x = (x_1, x_2)$$，核函数形式为$$k(x, x') = \theta_0 exp(- \frac{1}{2} \sum^2 _ {i=1} \eta_i (x_i - x_i')^2)。观察由y(x)生成的样本点，可以看到$$\eta_i$$变小的时候，函数对于输入变的越发不敏感。通过最大似然来决定这些参数，可以用来决定输入变量对预测分布的影响大小。对于影响特别小的输入，实际中我们可以考虑去掉这些值。

###4、高斯过程分类
接下来，我们扩展到分类。对于给定的训练数据，现在要预测一个新输入向量对应的目标的后验概率，而概率值是介于0-1之间,而高斯过程的值是实数值，所以需要用一些非线性激活函数，比如sigmoid。

这里以二元分类为例，t取值0或1, 我们的高斯过程定义为：y = \sigma(a(x))，这里的a(x)是来自高斯过程。这里$$p(t \mid a) = \sigma(a)^t (1- \sigma(a))^{1-t}$$,是伯努力分布，而$$p(a _ {N+1}) = N(a _{N+1} \mid 0, C _ {N+1})$$，跟回归是一样的。但与回归不同的是，协方差矩阵不再包含噪声项，因为我们假设了训练数据的全部都是正确预测了。但是为了计算出来方便，我们还是引入类似于噪声的项，由参数v控制。因此协方差矩阵$$C _ {N+1}$$的元素为：$$C(x_n, x_m) = k(x_n, x_m) + v \sigma _ {nm}$$。我们可以假设k是由一组参数$$\theta$$决定，之后会看到如何从数据中学习这些参数。

因此预测分布为:

$$p(t_ {N+1} = 1 \mid t) = \int p(t _ {N+1} = 1 \mid a _ {N+1}) p(a_ {N+1} \mid t) d a _ {N+1} = \int \sigma(a_ {N+1}) p(a_ {N+1} \mid t) d a _ {N+1}$$

这里的积分是比较难处理的，因此需要考虑使用近似的方法。主要有三种近似方法，第一是基于变分推断(variational inference)，具体到了第十章会有解释。其次是期望传导(expectation propagation)。最后是拉普拉斯近似。书中主要介绍了拉普拉斯近似求解。

拉普拉斯近似在前面章节已经介绍过了。推导没有细看。

最后我们看一下与神经网络的联系。在神经网络中，可以使用M个隐含层来表示一系列函数。足够大的M，在两层神经网络系统中，可以以任意精度逼近任何函数。在使用最大似然框架中，为了避免过拟合，需要限制一下M的个数。而在贝叶斯框架中，是不需要限制的。但是需要引入先验分布。在引入w的先验分布后，在M趋近于无穷大时，神经网络生成的函数的分布趋向于高斯过程。需要注意的是，在这样的限制下，输出变量之间是相互独立的。但是某些这些神经网络模型中，输出是共享部分隐含层的（相互之间借用了彼此的统计特性，borrow statistical strength）。

我们看到高斯过程是由协方差函数(核函数)决定的。而神经网络中两类隐含层激活函数(probit和高斯)，对于协方差是没有统计特性的，即无法表达x-x'的差异，因此导致高斯权重先验的均值聚集到0点，打破了权重空间的变化不变性(translation invariance),这个需要阅读William的论文。
